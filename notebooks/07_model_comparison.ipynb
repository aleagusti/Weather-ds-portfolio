{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cd61a62",
   "metadata": {},
   "source": [
    "Persistence Baseline & Model Comparison\n",
    "\n",
    "Context\n",
    "\n",
    "The project currently includes:\n",
    "\t‚Ä¢\ta fully reproducible end-to-end pipeline\n",
    "\t‚Ä¢\tstructural and content validation tests (ISSUE #6 closed)\n",
    "\t‚Ä¢\tseparate notebooks for each model family\n",
    "\n",
    "Before selecting a final production model (selected_model.py), it is necessary to compare all ML models against a trivial baseline to properly contextualize performance gains.\n",
    "\n",
    "‚∏ª\n",
    "\n",
    "Objective\n",
    "\n",
    "Establish a persistence baseline to answer the key question:\n",
    "\n",
    "Does the ML model actually improve over something trivial?\n",
    "\n",
    "Baseline definition:\n",
    "≈∑(t) = y(t‚àí1)\n",
    "\n",
    "This baseline serves as a minimum scientific benchmark.\n",
    "Where the baseline lives\n",
    "\t‚Ä¢\tNotebook: 06_model_comparison.ipynb\n",
    "\t‚Ä¢\t‚ùå Not part of the automated pipeline\n",
    "\t‚Ä¢\t‚ùå Not implemented in models/\n",
    "\t‚Ä¢\t‚ùå Not covered by pytest tests\n",
    "\n",
    "The baseline is analytical, not production code.\n",
    "Key rules\n",
    "\t‚Ä¢\tUse the same temporal split as ML models\n",
    "\t‚Ä¢\tEvaluate on the same test set\n",
    "\t‚Ä¢\tUse the same metrics as ML\n",
    "\t‚Ä¢\tDo not tune or optimize the baseline\n",
    "\n",
    "‚∏ª\n",
    "\n",
    "Metrics to compare\n",
    "\t‚Ä¢\tMAE\n",
    "\t‚Ä¢\tRMSE\n",
    "\t‚Ä¢\tR¬≤\n",
    "\n",
    "No additional metrics at this stage.\n",
    "\n",
    "‚∏ª\n",
    "\n",
    "Outputs to persist (minimum)\n",
    "\n",
    "Saved under data/results/:\n",
    "\t‚Ä¢\tmetrics_baseline_vXXX.csv\n",
    "\t‚Ä¢\tcolumns: model, MAE, RMSE, R2\n",
    "\t‚Ä¢\tmodel = \"persistence\"\n",
    "\n",
    "Optional:\n",
    "\t‚Ä¢\tpredictions_baseline_vXXX.csv\n",
    "\t‚Ä¢\tdate, y_true, y_pred, error\n",
    "\n",
    "‚∏ª\n",
    "\n",
    "Central comparison table (core artifact)\n",
    "model\n",
    "MAE\n",
    "RMSE\n",
    "R2\n",
    "persistence\n",
    "‚Ä¶\n",
    "‚Ä¶\n",
    "‚Ä¶\n",
    "random_forest\n",
    "‚Ä¶\n",
    "‚Ä¶\n",
    "‚Ä¶\n",
    "xgboost\n",
    "‚Ä¶\n",
    "‚Ä¶\n",
    "‚Ä¶\n",
    "neural_net\n",
    "‚Ä¶\n",
    "‚Ä¶\n",
    "‚Ä¶\n",
    "\n",
    "Relative improvement vs baseline\n",
    "\n",
    "Add a derived column:\n",
    "improvement_vs_baseline_% = (RMSE_baseline - RMSE_model) / RMSE_baseline\n",
    "Improvements are always computed against the baseline, never between ML models.\n",
    "\n",
    "‚∏ª\n",
    "\n",
    "Suggested visualization (optional)\n",
    "\t‚Ä¢\tBar plot of RMSE by model\n",
    "\t‚Ä¢\tBaseline clearly highlighted\n",
    "\n",
    "‚∏ª\n",
    "\n",
    "Expected conclusion (Markdown)\n",
    "\n",
    "The notebook should close with an explicit statement such as:\n",
    "\n",
    "‚ÄúModel X reduces RMSE by approximately Y% compared to the persistence baseline, indicating that it captures non-trivial temporal and meteorological patterns beyond simple persistence.‚Äù\n",
    "\n",
    "This text will later be reused in:\n",
    "\t‚Ä¢\tREADME\n",
    "\t‚Ä¢\tmodel selection justification\n",
    "\t‚Ä¢\ttechnical interviews / reviews\n",
    "\n",
    "‚∏ª\n",
    "\n",
    "Criterion to move to selected_model.py\n",
    "\n",
    "Proceed only when:\n",
    "\t‚Ä¢\tall candidate models have been evaluated\n",
    "\t‚Ä¢\tthe comparison table is complete\n",
    "\t‚Ä¢\timprovement over the baseline is clear and defensible\n",
    "\n",
    "Only then:\n",
    "\t‚Ä¢\timplement models/selected_model.py\n",
    "\t‚Ä¢\tfreeze the modeling decision\n",
    "\n",
    "What NOT to do\n",
    "\t‚Ä¢\tDo not integrate the baseline into the pipeline\n",
    "\t‚Ä¢\tDo not force ML to ‚Äúwin‚Äù\n",
    "\t‚Ä¢\tDo not add unnecessary complexity\n",
    "\t‚Ä¢\tDo not select a model without this comparisonStatus\n",
    "\n",
    "üìå Bookmark saved ‚Äî development deferred\n",
    "To be revisited once all experimentation notebooks are finalized."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
